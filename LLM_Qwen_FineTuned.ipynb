{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwUc-iu95SjS",
        "outputId": "33277a5b-f71f-468a-b01c-3893f20b7ed6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1DP9bLC4uZs",
        "outputId": "2944d548-2ac0-410a-947d-d4fccd6262dd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "✅ Loaded: 28812 train, 4194 dev, 8161 test samples.\n",
            "Disaster types: ['Cyclone', 'Earthquake', 'Flood', 'Hurricane', 'Wildfire']\n",
            "Humanitarian types: ['caution_and_advice', 'displaced_people_and_evacuations', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'missing_or_found_people', 'not_humanitarian', 'other_relevant_information', 'requests_or_urgent_needs', 'rescue_volunteering_or_donation_effort', 'sympathy_and_support']\n",
            "Loading base model (this may take a while)...\n",
            "PEFT/LoRA modules injected.\n",
            "trainable params: 1,089,536 || all params: 1,544,803,840 || trainable%: 0.0705\n",
            "\n",
            "=== Epoch 1/3 ===\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 avg loss: 1.7164\n",
            "\n",
            "=== Epoch 2/3 ===\n",
            "Epoch 2 avg loss: 0.7690\n",
            "\n",
            "=== Epoch 3/3 ===\n",
            "Epoch 3 avg loss: 0.6927\n",
            "\n",
            "===== Disaster Classification =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Cyclone       1.00      0.98      0.99       779\n",
            "  Earthquake       0.95      0.96      0.96      1384\n",
            "       Flood       0.94      0.89      0.91       259\n",
            "   Hurricane       0.98      0.98      0.98      5438\n",
            "    Wildfire       0.99      0.98      0.99       301\n",
            "\n",
            "    accuracy                           0.98      8161\n",
            "   macro avg       0.97      0.96      0.97      8161\n",
            "weighted avg       0.98      0.98      0.98      8161\n",
            "\n",
            "Accuracy: 0.976718539394682\n",
            "\n",
            "===== Humanitarian Classification =====\n",
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "                    caution_and_advice       0.73      0.61      0.66       433\n",
            "      displaced_people_and_evacuations       0.91      0.92      0.92       347\n",
            "     infrastructure_and_utility_damage       0.84      0.88      0.86      1147\n",
            "                injured_or_dead_people       0.92      0.96      0.94       840\n",
            "               missing_or_found_people       0.80      0.78      0.79        36\n",
            "                      not_humanitarian       0.51      0.31      0.38       376\n",
            "            other_relevant_information       0.60      0.62      0.61      1455\n",
            "              requests_or_urgent_needs       0.67      0.48      0.56       324\n",
            "rescue_volunteering_or_donation_effort       0.82      0.91      0.86      2227\n",
            "                  sympathy_and_support       0.87      0.81      0.84       976\n",
            "\n",
            "                              accuracy                           0.79      8161\n",
            "                             macro avg       0.77      0.73      0.74      8161\n",
            "                          weighted avg       0.78      0.79      0.78      8161\n",
            "\n",
            "Accuracy: 0.7856880284278888\n",
            "\n",
            "✅ Training complete. Artifacts saved to: /content/drive/MyDrive/humAID_Qwen2_lora_fixed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Qwen2-1.5B Multitask Fine-tuning with LoRA (Corrected)\n",
        "# ============================================================\n",
        "# Run in a single Colab cell. Adjust DATA_ROOT and OUT_DIR as needed.\n",
        "\n",
        "!pip install -q -U transformers accelerate peft bitsandbytes datasets safetensors\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------\n",
        "# User config\n",
        "# -------------------------\n",
        "DATA_ROOT = \"/content/drive/MyDrive/humAID_dataset\"   # change if needed\n",
        "OUT_DIR = \"/content/drive/MyDrive/humAID_Qwen2_lora_fixed\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "BASE_MODEL = \"Qwen/Qwen2-1.5B\"\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_STEPS = 0\n",
        "SEED = 42\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# Data loading (same as your original pipeline)\n",
        "# ============================================================\n",
        "def read_messy_tsv(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    split_lines = [line.strip().split('\\t')[-3:] for line in lines]\n",
        "    data = split_lines[1:]\n",
        "    df = pd.DataFrame(data, columns=['tweet_text', 'class_label', 'disaster_type'])\n",
        "    df = df[~df['tweet_text'].str.contains('tweet_text', case=False, na=False)]\n",
        "    df = df[~df['class_label'].str.contains('class_label', case=False, na=False)]\n",
        "    df = df[~df['disaster_type'].str.contains('disaster_type', case=False, na=False)]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def load_all_splits(base_folder):\n",
        "    train_list, dev_list, test_list = [], [], []\n",
        "    for subfolder in os.listdir(base_folder):\n",
        "        subpath = os.path.join(base_folder, subfolder)\n",
        "        if not os.path.isdir(subpath):\n",
        "            continue\n",
        "        for split in ['train', 'dev', 'test']:\n",
        "            file_path = os.path.join(subpath, f\"{subfolder}_{split}.tsv\")\n",
        "            if os.path.exists(file_path):\n",
        "                df = read_messy_tsv(file_path)\n",
        "                if split == 'train':\n",
        "                    train_list.append(df)\n",
        "                elif split == 'dev':\n",
        "                    dev_list.append(df)\n",
        "                else:\n",
        "                    test_list.append(df)\n",
        "    train_df = pd.concat(train_list, ignore_index=True) if train_list else pd.DataFrame()\n",
        "    dev_df = pd.concat(dev_list, ignore_index=True) if dev_list else pd.DataFrame()\n",
        "    test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame()\n",
        "    print(f\"✅ Loaded: {len(train_df)} train, {len(dev_df)} dev, {len(test_df)} test samples.\")\n",
        "    return train_df, dev_df, test_df\n",
        "\n",
        "train_df, dev_df, test_df = load_all_splits(DATA_ROOT)\n",
        "\n",
        "for df in [train_df, dev_df, test_df]:\n",
        "    df.rename(columns={'tweet_text': 'text', 'class_label': 'text_humanitarian'}, inplace=True)\n",
        "    df.dropna(subset=['text', 'text_humanitarian', 'disaster_type'], inplace=True)\n",
        "\n",
        "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
        "\n",
        "# Label encoding\n",
        "disaster_encoder = LabelEncoder()\n",
        "human_encoder = LabelEncoder()\n",
        "train_df['disaster_label'] = disaster_encoder.fit_transform(train_df['disaster_type'])\n",
        "train_df['human_label'] = human_encoder.fit_transform(train_df['text_humanitarian'])\n",
        "\n",
        "test_df['disaster_label'] = test_df['disaster_type'].map(\n",
        "    lambda x: disaster_encoder.transform([x])[0] if x in disaster_encoder.classes_ else -1\n",
        ")\n",
        "test_df['human_label'] = test_df['text_humanitarian'].map(\n",
        "    lambda x: human_encoder.transform([x])[0] if x in human_encoder.classes_ else -1\n",
        ")\n",
        "test_df = test_df[(test_df['disaster_label'] != -1) & (test_df['human_label'] != -1)]\n",
        "\n",
        "num_labels_disaster = len(disaster_encoder.classes_)\n",
        "num_labels_human = len(human_encoder.classes_)\n",
        "print(\"Disaster types:\", list(disaster_encoder.classes_))\n",
        "print(\"Humanitarian types:\", list(human_encoder.classes_))\n",
        "\n",
        "# ============================================================\n",
        "# Tokenizer and dataset\n",
        "# ============================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class CrisisDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df['text'].tolist()\n",
        "        self.disaster = df['disaster_label'].tolist()\n",
        "        self.human = df['human_label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(0),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
        "            'disaster_label': torch.tensor(self.disaster[idx], dtype=torch.long),\n",
        "            'human_label': torch.tensor(self.human[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = CrisisDataset(train_df)\n",
        "test_dataset = CrisisDataset(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ============================================================\n",
        "# Load base model in 4-bit + prepare for LoRA\n",
        "# ============================================================\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "print(\"Loading base model (this may take a while)...\")\n",
        "base_peft_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare for k-bit training\n",
        "base_peft_model = prepare_model_for_kbit_training(base_peft_model)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(base_peft_model, lora_config)\n",
        "print(\"PEFT/LoRA modules injected.\")\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "# ============================================================\n",
        "# Multitask wrapper (robust attribute access for wrapped model)\n",
        "# ============================================================\n",
        "class QwenForMultiTask(nn.Module):\n",
        "    def __init__(self, peft_model, hidden_size, n_disaster, n_human):\n",
        "        super().__init__()\n",
        "        # store peft model (this contains the quantized base and LoRA adapters)\n",
        "        self.peft_model = peft_model\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.disaster_head = nn.Linear(hidden_size, n_disaster)\n",
        "        self.human_head = nn.Linear(hidden_size, n_human)\n",
        "\n",
        "    def _get_base_transformer(self):\n",
        "        \"\"\"\n",
        "        Return the actual transformer module inside the HF wrapper.\n",
        "        This is robust across different wrappers: try common attribute names.\n",
        "        \"\"\"\n",
        "        # peft_model may have attributes like .model, .base_model, .transformer, etc.\n",
        "        cand = getattr(self.peft_model, \"model\", None)\n",
        "        if cand is None:\n",
        "            cand = getattr(self.peft_model, \"base_model\", None)\n",
        "        if cand is None:\n",
        "            cand = getattr(self.peft_model, \"transformer\", None)\n",
        "        if cand is None:\n",
        "            # fallback to peft_model itself\n",
        "            cand = self.peft_model\n",
        "        return cand\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        base = self._get_base_transformer()\n",
        "\n",
        "        # Ensure we disable cache and request hidden states\n",
        "        outputs = base(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # For decoder-only models, take last layer's hidden states and pool last token\n",
        "        hidden_states = outputs.hidden_states  # tuple of layers\n",
        "        last_hidden = hidden_states[-1]        # [B, seq_len, hidden]\n",
        "        pooled = last_hidden[:, -1, :]         # last-token embedding\n",
        "        pooled = self.dropout(pooled)\n",
        "\n",
        "        d_logits = self.disaster_head(pooled)\n",
        "        h_logits = self.human_head(pooled)\n",
        "        return d_logits, h_logits\n",
        "\n",
        "# find hidden size robustly\n",
        "# Many wrappers: peft_model.base_model.model.config.hidden_size or peft_model.config.hidden_size\n",
        "if hasattr(peft_model, \"base_model\") and hasattr(peft_model.base_model, \"model\"):\n",
        "    hidden_size = peft_model.base_model.model.config.hidden_size\n",
        "elif hasattr(peft_model, \"base_model\") and hasattr(peft_model.base_model, \"config\"):\n",
        "    hidden_size = peft_model.base_model.config.hidden_size\n",
        "elif hasattr(peft_model, \"model\") and hasattr(peft_model.model, \"config\"):\n",
        "    hidden_size = peft_model.model.config.hidden_size\n",
        "else:\n",
        "    hidden_size = peft_model.config.hidden_size\n",
        "\n",
        "multi_model = QwenForMultiTask(peft_model, hidden_size, num_labels_disaster, num_labels_human).to(device)\n",
        "\n",
        "# ============================================================\n",
        "# Optimizer (only trainable params: LoRA + heads)\n",
        "# ============================================================\n",
        "trainable_params = [p for p in multi_model.parameters() if p.requires_grad]\n",
        "optimizer = AdamW(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
        "\n",
        "# Use new torch.amp API if CUDA\n",
        "if device.type == \"cuda\":\n",
        "    scaler = torch.amp.GradScaler(\"cuda\")\n",
        "else:\n",
        "    scaler = None  # no scaler for CPU\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ============================================================\n",
        "# Training loop\n",
        "# ============================================================\n",
        "multi_model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
        "    epoch_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        d_labels = batch['disaster_label'].to(device)\n",
        "        h_labels = batch['human_label'].to(device)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                d_logits, h_logits = multi_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss_d = criterion(d_logits, d_labels)\n",
        "                loss_h = criterion(h_logits, h_labels)\n",
        "                loss = loss_d + loss_h\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # CPU fallback\n",
        "            d_logits, h_logits = multi_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss_d = criterion(d_logits, d_labels)\n",
        "            loss_h = criterion(h_logits, h_labels)\n",
        "            loss = loss_d + loss_h\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# Evaluation\n",
        "# ============================================================\n",
        "multi_model.eval()\n",
        "true_d_all, pred_d_all = [], []\n",
        "true_h_all, pred_h_all = [], []\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        d_labels = batch['disaster_label'].to(device)\n",
        "        h_labels = batch['human_label'].to(device)\n",
        "\n",
        "        d_logits, h_logits = multi_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        d_preds = torch.argmax(d_logits, dim=1)\n",
        "        h_preds = torch.argmax(h_logits, dim=1)\n",
        "\n",
        "        true_d_all.extend(d_labels.cpu().tolist())\n",
        "        pred_d_all.extend(d_preds.cpu().tolist())\n",
        "        true_h_all.extend(h_labels.cpu().tolist())\n",
        "        pred_h_all.extend(h_preds.cpu().tolist())\n",
        "\n",
        "        for i in range(len(d_preds)):\n",
        "            results.append({\n",
        "                \"text\": tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True),\n",
        "                \"true_disaster\": disaster_encoder.inverse_transform([d_labels[i].cpu().item()])[0],\n",
        "                \"pred_disaster\": disaster_encoder.inverse_transform([d_preds[i].cpu().item()])[0],\n",
        "                \"true_human\": human_encoder.inverse_transform([h_labels[i].cpu().item()])[0],\n",
        "                \"pred_human\": human_encoder.inverse_transform([h_preds[i].cpu().item()])[0],\n",
        "            })\n",
        "\n",
        "# ============================================================\n",
        "# Save artifacts: LoRA adapters + heads + tokenizer + label maps\n",
        "# ============================================================\n",
        "# Save PEFT adapters (LoRA)\n",
        "peft_model.save_pretrained(OUT_DIR)\n",
        "\n",
        "# Save the small heads' weights separately\n",
        "heads_state = {\n",
        "    \"disaster_head\": multi_model.disaster_head.state_dict(),\n",
        "    \"human_head\": multi_model.human_head.state_dict()\n",
        "}\n",
        "torch.save(heads_state, os.path.join(OUT_DIR, \"task_heads.pth\"))\n",
        "\n",
        "# Save tokenizer and label maps\n",
        "tokenizer.save_pretrained(OUT_DIR)\n",
        "with open(os.path.join(OUT_DIR, \"label_maps.json\"), \"w\") as f:\n",
        "    json.dump({\n",
        "        \"disaster_labels\": dict(enumerate(disaster_encoder.classes_)),\n",
        "        \"human_labels\": dict(enumerate(human_encoder.classes_))\n",
        "    }, f)\n",
        "\n",
        "pd.DataFrame(results).to_csv(os.path.join(OUT_DIR, \"test_predictions.csv\"), index=False)\n",
        "\n",
        "# Reports\n",
        "print(\"\\n===== Disaster Classification =====\")\n",
        "print(classification_report(true_d_all, pred_d_all, target_names=disaster_encoder.classes_))\n",
        "print(\"Accuracy:\", accuracy_score(true_d_all, pred_d_all))\n",
        "\n",
        "print(\"\\n===== Humanitarian Classification =====\")\n",
        "print(classification_report(true_h_all, pred_h_all, target_names=human_encoder.classes_))\n",
        "print(\"Accuracy:\", accuracy_score(true_h_all, pred_h_all))\n",
        "\n",
        "print(f\"\\n✅ Training complete. Artifacts saved to: {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfTJuy1C6BY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}