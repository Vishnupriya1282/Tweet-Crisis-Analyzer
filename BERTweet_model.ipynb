{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# humAID with testing and dev(validations)\n",
        "# STEP 1: Imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import os\n",
        "import json\n",
        "\n",
        "# STEP 2: Load dataset (train, dev, test) using actual disaster_type from TSV\n",
        "def read_messy_tsv(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    split_lines = [line.strip().split('\\t')[-3:] for line in lines]\n",
        "\n",
        "    data = split_lines[1:]\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['tweet_text', 'class_label', 'disaster_type'])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['tweet_text', 'class_label', 'disaster_type'])\n",
        "\n",
        "    df = df[~df['tweet_text'].str.contains('tweet_text', case=False, na=False)]\n",
        "    df = df[~df['class_label'].str.contains('class_label', case=False, na=False)]\n",
        "    df = df[~df['disaster_type'].str.contains('disaster_type', case=False, na=False)]\n",
        "\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/humAID_dataset/canade_wildfires_2016/canada_wildfires_2016_train.tsv\"\n",
        "df = read_messy_tsv(file_path)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "data_folder = \"/content/drive/MyDrive/humAID_dataset/\"\n",
        "\n",
        "def load_all_splits(base_folder):\n",
        "    train_list, dev_list, test_list = [], [], []\n",
        "    for subfolder in os.listdir(base_folder):\n",
        "        subpath = os.path.join(base_folder, subfolder)\n",
        "        if not os.path.isdir(subpath):\n",
        "            continue\n",
        "\n",
        "        for split in ['train', 'dev', 'test']:\n",
        "            file_path = os.path.join(subpath, f\"{subfolder}_{split}.tsv\")\n",
        "            if os.path.exists(file_path):\n",
        "                df = read_messy_tsv(file_path)\n",
        "                if split == 'train':\n",
        "                    train_list.append(df)\n",
        "                elif split == 'dev':\n",
        "                    dev_list.append(df)\n",
        "                else:\n",
        "                    test_list.append(df)\n",
        "\n",
        "    train_df = pd.concat(train_list, ignore_index=True) if train_list else pd.DataFrame()\n",
        "    dev_df = pd.concat(dev_list, ignore_index=True) if dev_list else pd.DataFrame()\n",
        "    test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame()\n",
        "\n",
        "    print(f\" Loaded: {len(train_df)} train, {len(dev_df)} dev, {len(test_df)} test samples.\")\n",
        "    print(\"Columns detected:\", list(train_df.columns))\n",
        "    return train_df, dev_df, test_df\n",
        "\n",
        "\n",
        "train_df, dev_df, test_df = load_all_splits(data_folder)\n",
        "\n",
        "# STEP 3: Clean columns\n",
        "for df in [train_df, dev_df, test_df]:\n",
        "    if len(df) > 0 and df.iloc[0]['tweet_text'] == 'tweet_text':\n",
        "        df.drop(index=0, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for df in [train_df, dev_df, test_df]:\n",
        "    df.rename(columns={'tweet_text': 'text', 'class_label': 'text_humanitarian'}, inplace=True)\n",
        "    df.dropna(subset=['text', 'text_humanitarian', 'disaster_type'], inplace=True)\n",
        "\n",
        "\n",
        "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
        "\n",
        "for df in [train_df, dev_df, test_df]:\n",
        "    df.rename(columns={'tweet_text': 'text', 'class_label': 'text_humanitarian'}, inplace=True)\n",
        "    df.dropna(subset=['text', 'text_humanitarian', 'disaster_type'], inplace=True)\n",
        "\n",
        "\n",
        "train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
        "\n",
        "# STEP 4: Encode Labels\n",
        "disaster_encoder = LabelEncoder()\n",
        "human_encoder = LabelEncoder()\n",
        "\n",
        "train_df['disaster_label'] = disaster_encoder.fit_transform(train_df['disaster_type'])\n",
        "train_df['human_label'] = human_encoder.fit_transform(train_df['text_humanitarian'])\n",
        "\n",
        "test_df['disaster_label'] = test_df['disaster_type'].map(\n",
        "    lambda x: disaster_encoder.transform([x])[0] if x in disaster_encoder.classes_ else -1\n",
        ")\n",
        "test_df['human_label'] = test_df['text_humanitarian'].map(\n",
        "    lambda x: human_encoder.transform([x])[0] if x in human_encoder.classes_ else -1\n",
        ")\n",
        "\n",
        "test_df = test_df[(test_df['disaster_label'] != -1) & (test_df['human_label'] != -1)]\n",
        "\n",
        "num_labels_disaster = len(disaster_encoder.classes_)\n",
        "num_labels_human = len(human_encoder.classes_)\n",
        "\n",
        "print(\"\\nDisaster types:\", list(disaster_encoder.classes_))\n",
        "print(\"Humanitarian types:\", list(human_encoder.classes_))\n",
        "\n",
        "# STEP 5: Dataset Class\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "\n",
        "class CrisisDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df['text'].tolist()\n",
        "        self.disaster = df['disaster_label'].tolist()\n",
        "        self.human = df['human_label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'disaster_label': torch.tensor(self.disaster[idx]),\n",
        "            'human_label': torch.tensor(self.human[idx])\n",
        "        }\n",
        "\n",
        "train_dataset = CrisisDataset(train_df)\n",
        "test_dataset = CrisisDataset(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# STEP 6: Define Model\n",
        "class BertweetMultiHead(nn.Module):\n",
        "    def __init__(self, base_model_name, num_labels_disaster, num_labels_human):\n",
        "        super().__init__()\n",
        "        self.bertweet = AutoModel.from_pretrained(base_model_name)\n",
        "        hidden_size = self.bertweet.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.disaster_head = nn.Linear(hidden_size, num_labels_disaster)\n",
        "        self.human_head = nn.Linear(hidden_size, num_labels_human)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bertweet(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.pooler_output\n",
        "        pooled = self.dropout(pooled)\n",
        "        return self.disaster_head(pooled), self.human_head(pooled)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertweetMultiHead(\"vinai/bertweet-base\", num_labels_disaster, num_labels_human).to(device)\n",
        "\n",
        "# STEP 7: Training Setup\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 10\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
        "\n",
        "# STEP 8: Training Loop\n",
        "for epoch in range(epochs):\n",
        "    print(\"epoch: \"+ str(epoch) +\" is running....\\n\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        disaster_label = batch['disaster_label'].to(device)\n",
        "        human_label = batch['human_label'].to(device)\n",
        "\n",
        "        d_logits, h_logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(d_logits, disaster_label) + criterion(h_logits, human_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "model.eval()\n",
        "results = []\n",
        "true_disaster_all, pred_disaster_all = [], []\n",
        "true_human_all, pred_human_all = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        disaster_label = batch['disaster_label'].to(device)\n",
        "        human_label = batch['human_label'].to(device)\n",
        "\n",
        "        d_logits, h_logits = model(input_ids, attention_mask)\n",
        "        d_preds = torch.argmax(d_logits, axis=1)\n",
        "        h_preds = torch.argmax(h_logits, axis=1)\n",
        "\n",
        "        true_disaster_all.extend(disaster_label.cpu().tolist())\n",
        "        pred_disaster_all.extend(d_preds.cpu().tolist())\n",
        "        true_human_all.extend(human_label.cpu().tolist())\n",
        "        pred_human_all.extend(h_preds.cpu().tolist())\n",
        "\n",
        "        for i in range(len(d_preds)):\n",
        "            results.append({\n",
        "                \"text\": tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True),\n",
        "                \"true_disaster\": disaster_encoder.inverse_transform([disaster_label[i].cpu().item()])[0],\n",
        "                \"pred_disaster\": disaster_encoder.inverse_transform([d_preds[i].cpu().item()])[0],\n",
        "                \"true_human\": human_encoder.inverse_transform([human_label[i].cpu().item()])[0],\n",
        "                \"pred_human\": human_encoder.inverse_transform([h_preds[i].cpu().item()])[0]\n",
        "            })\n",
        "\n",
        "# STEP 10: Save Test Predictions\n",
        "results_df = pd.DataFrame(results)\n",
        "results_csv_path = \"/content/drive/MyDrive/humAID_dataset_test_predictions.csv\"\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "print(f\"\\nSaved full test predictions to: {results_csv_path}\")\n",
        "\n",
        "# STEP 11: Classification Reports\n",
        "print(\"\\n===== Disaster Type Classification =====\")\n",
        "print(classification_report(true_disaster_all, pred_disaster_all, target_names=disaster_encoder.classes_))\n",
        "print(\"Accuracy:\", accuracy_score(true_disaster_all, pred_disaster_all))\n",
        "\n",
        "print(\"\\n===== Humanitarian Type Classification =====\")\n",
        "print(classification_report(true_human_all, pred_human_all, target_names=human_encoder.classes_))\n",
        "print(\"Accuracy:\", accuracy_score(true_human_all, pred_human_all))\n",
        "\n",
        "# STEP 12: Save Model and Label Maps\n",
        "save_path = \"/content/drive/MyDrive/humAID_dataset_BERTweet_Model\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), f\"{save_path}/bertweet_multitask.pth\")\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "with open(f\"{save_path}/label_maps.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"disaster_labels\": dict(enumerate(disaster_encoder.classes_)),\n",
        "        \"human_labels\": dict(enumerate(human_encoder.classes_))\n",
        "    }, f)\n",
        "\n",
        "print(f\"\\nModel, tokenizer, and label maps saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "JfIwbfSi4V70"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}